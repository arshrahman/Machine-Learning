{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "An example of TFRecord data loading, preprocessing and RNN training.\n",
    "    - Creates threads to read TFRecord files from disk, decode and preprocess.\n",
    "    - Crops and resizes the RGB frames, i.e., images, (32x32) and flatten: 1024 dimensional representation vector.\n",
    "    - Builds recurrent 2-layer LSTM model\n",
    "    - Trains the model on flattened image vectors.\n",
    "\n",
    "You can use 2D CNN for representation learning on images or 3D volumetric CNN on multiple frames. You should find out how to stack CNN and RNN networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/arshan/project/codes/runs/1497438780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "# Get from dataset.\n",
    "config['num_test_samples'] = 2174\n",
    "config['num_validation_samples'] = 1765\n",
    "config['num_training_samples'] = 5722\n",
    "\n",
    "# Good RGB: 16, 1e-1, 0.5\n",
    "# GOOD RGBD: 16, 1e-1, 0.8\n",
    "config['batch_size'] = 16\n",
    "config['learning_rate'] = 1e-3\n",
    "config['momentum'] = 0.8\n",
    "# Learning rate is annealed exponentally in 'exponential' case. Don't forget to change annealing configuration in the code.\n",
    "config['learning_rate_type'] = 'fixed' #'exponential'\n",
    "\n",
    "config['num_steps_per_epoch'] = int(config['num_training_samples']/config['batch_size'])\n",
    "\n",
    "config['num_epochs'] = 1000\n",
    "config['evaluate_every_step'] = config['num_steps_per_epoch']*5\n",
    "config['checkpoint_every_step'] = config['num_steps_per_epoch']*5\n",
    "config['num_validation_steps'] = int(config['num_validation_samples']/config['batch_size'])\n",
    "config['print_every_step'] = config['num_steps_per_epoch']\n",
    "config['log_dir'] = './runs/'\n",
    "\n",
    "config['img_height'] = 80\n",
    "config['img_width'] = 80\n",
    "config['img_num_channels'] = 3\n",
    "config['skeleton_size'] = 180\n",
    "\n",
    "# CNN model parameters\n",
    "config['cnn'] = {}\n",
    "config['cnn']['cnn_filters'] = [32,64,64,128] # Number of filters for every convolutional layer.\n",
    "config['cnn']['num_hidden_units'] = 512 # Number of output units, i.e. representation size.\n",
    "config['cnn']['dropout_rate'] = 0.6\n",
    "config['cnn']['initializer'] = tf.contrib.layers.xavier_initializer()\n",
    "# RNN model parameters\n",
    "config['rnn'] = {}\n",
    "config['rnn']['num_hidden_units'] = 512 # Number of units in an LSTM cell.\n",
    "config['rnn']['num_layers'] = 1 # Number of LSTM stack.\n",
    "config['rnn']['num_class_labels'] = 20\n",
    "config['rnn']['initializer'] = tf.contrib.layers.xavier_initializer()\n",
    "config['rnn']['batch_size'] = config['batch_size']\n",
    "config['rnn']['loss_type'] = 'average' # or 'last_step' # In the case of 'average', average of all time-steps is used instead of the last time-step.\n",
    "\n",
    "config['ip_queue_capacity'] = config['batch_size']*50\n",
    "config['ip_num_read_threads'] = 6\n",
    "\n",
    "config['train_data_dir'] = \"../train\"\n",
    "config['train_file_format'] = \"dataTrain_%d.tfrecords\"\n",
    "config['train_file_ids'] = list(range(1,41))\n",
    "config['valid_data_dir'] = \"../validation\"\n",
    "config['valid_file_format'] = \"dataValidation_%d.tfrecords\"\n",
    "config['valid_file_ids'] = list(range(1,16))\n",
    "\n",
    "\n",
    "# Create a unique output directory for this experiment.\n",
    "timestamp = str(int(time.time()))\n",
    "config['model_dir'] = os.path.abspath(os.path.join(config['log_dir'], timestamp))\n",
    "print(\"Writing to {}\\n\".format(config['model_dir']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_op(image_op, config):\n",
    "    \n",
    "    with tf.name_scope(\"preprocessing\"):\n",
    "        \n",
    "        # split input\n",
    "        rgb_op, dep_op, seg_op = image_op\n",
    "        \n",
    "        # get RGB image\n",
    "        rgb_op = tf.reshape(rgb_op, (config['img_height'], \n",
    "                                         config['img_width'], \n",
    "                                         config['img_num_channels'])\n",
    "                          )\n",
    "        # get segmentation image\n",
    "        seg_op = tf.reshape(seg_op, (config['img_height'], \n",
    "                                         config['img_width'], \n",
    "                                         config['img_num_channels'])\n",
    "                          )\n",
    "        # threshold segmentation & cast to int\n",
    "        seg_op = seg_op > 100\n",
    "        seg_op = tf.cast(seg_op, tf.uint8)\n",
    "        \n",
    "        # mask rgb with segmentation\n",
    "        rgb_op = tf.multiply(rgb_op, seg_op)\n",
    "        \n",
    "        # slice segmentation for depth masking\n",
    "        seg_op = tf.slice(seg_op, [0,0,0], [80,80,1])\n",
    "        \n",
    "        # get depth image\n",
    "        dep_op = tf.reshape(dep_op, (config['img_height'], \n",
    "                                         config['img_width'], \n",
    "                                         1)\n",
    "                          )\n",
    "        # mask depth with segmentation\n",
    "        dep_op = tf.multiply(dep_op, seg_op)\n",
    "        \n",
    "        # Convert from RGB to grayscale.\n",
    "        rgb_op = tf.image.rgb_to_grayscale(rgb_op)\n",
    "        \n",
    "        # Integer to float.\n",
    "        rgb_op = tf.to_float(rgb_op)\n",
    "        dep_op = tf.to_float(dep_op)\n",
    "        \n",
    "        #rgb_op = tf.image.random_flip_left_right(rgb_op, seed = 123)\n",
    "        #dep_op = tf.image.random_flip_left_right(dep_op, seed = 123)\n",
    "        \n",
    "        # Crop\n",
    "        #image_op = tf.image.resize_image_with_crop_or_pad(image_op, 60, 60)\n",
    "        \n",
    "        # Resize operation requires 4D tensors (i.e., batch of images).\n",
    "        # Reshape the image so that it looks like a batch of one sample: [1,60,60,1]\n",
    "        #image_op = tf.expand_dims(image_op, 0)\n",
    "        # Resize\n",
    "        #image_op = tf.image.resize_bilinear(image_op, np.asarray([32,32]))\n",
    "        # Reshape the image: [32,32,1]\n",
    "        #image_op = tf.squeeze(image_op, 0)\n",
    "         \n",
    "        rgb_op = tf.image.per_image_standardization(rgb_op)\n",
    "        image_op = tf.concat([rgb_op, dep_op], 2)\n",
    "        image_op = rgb_op\n",
    "        # Flatten image\n",
    "        #image_op = tf.reshape(image_op, [-1])\n",
    "    \n",
    "        return image_op\n",
    "\n",
    "def read_and_decode_sequence(filename_queue, config):\n",
    "    # Create a TFRecordReader.\n",
    "    readerOptions = tf.python_io.TFRecordOptions(compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    reader = tf.TFRecordReader(options=readerOptions)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    \n",
    "    # [height, width, numChannels] = [80, 80, 3]\n",
    "    with tf.name_scope(\"TFRecordDecoding\"):\n",
    "        context_encoded, sequence_encoded = tf.parse_single_sequence_example(\n",
    "                serialized_example,\n",
    "                # \"label\" and \"lenght\" are encoded as context features. \n",
    "                context_features={\n",
    "                    \"label\": tf.FixedLenFeature([], dtype=tf.int64),\n",
    "                    \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "                },\n",
    "                # \"depth\", \"rgb\", \"segmentation\", \"skeleton\" are encoded as sequence features.\n",
    "                sequence_features={\n",
    "                    \"depth\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"rgb\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"segmentation\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"skeleton\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                })\n",
    "\n",
    "        # Decode serialized rgb, depth & segmentation images \n",
    "        seq_rgb = tf.decode_raw(sequence_encoded['rgb'], tf.uint8)\n",
    "        seq_dep = tf.decode_raw(sequence_encoded['depth'], tf.uint8)\n",
    "        seq_seg = tf.decode_raw(sequence_encoded['segmentation'], tf.uint8)\n",
    "        \n",
    "        # Load labels\n",
    "        seq_label = context_encoded['label']\n",
    "        seq_label = seq_label - 1 # everything +1 for submission!\n",
    "        \n",
    "        # get sequence length\n",
    "        seq_len = tf.to_int32(context_encoded['length'])\n",
    "        # Output dimnesionality: [seq_len, height, width, numChannels]\n",
    "        \n",
    "        # combine seg,depth&rgb, then apply preprocessing\n",
    "        seq_all = [seq_rgb, seq_dep, seq_seg]\n",
    "        seq_all = tf.map_fn(lambda x: preprocessing_op(x, config),\n",
    "                                elems=seq_all,\n",
    "                                dtype=tf.float32,\n",
    "                                back_prop=False)\n",
    "        \n",
    "        return [seq_all, seq_label, seq_len]\n",
    "    \n",
    "\n",
    "def input_pipeline(filenames, config, name='input_pipeline', shuffle=True):\n",
    "    with tf.name_scope(name):\n",
    "        # Create a queue of TFRecord input files.\n",
    "        filename_queue = tf.train.string_input_producer(filenames, num_epochs=config['num_epochs'], shuffle=shuffle)\n",
    "        # Read the data from TFRecord files, decode and create a list of data samples by using threads.\n",
    "        sample_list = [read_and_decode_sequence(filename_queue, config) for _ in range(config['ip_num_read_threads'])]\n",
    "        \n",
    "        batch_rgb, batch_labels, batch_lens = tf.train.batch_join(sample_list,\n",
    "                                                    batch_size=config['batch_size'],\n",
    "                                                    capacity=config['ip_queue_capacity'],\n",
    "                                                    enqueue_many=False,\n",
    "                                                    dynamic_pad=True,\n",
    "                                                    allow_smaller_final_batch = False,\n",
    "                                                    name=\"batch_join_and_pad\")\n",
    "\n",
    "        return batch_rgb, batch_labels, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNNModel():\n",
    "    \"\"\"\n",
    "    Creates training and validation computational graphs.\n",
    "    Note that tf.variable_scope enables sharing the parameters so that both graphs share the parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, input_op, mode):\n",
    "        \"\"\"\n",
    "        Basic setup.\n",
    "        Args:\n",
    "          config: Object containing configuration parameters.\n",
    "        \"\"\"\n",
    "        assert mode in [\"training\", \"validation\"]\n",
    "        self.config = config\n",
    "        self.inputs = input_op\n",
    "        self.mode = mode\n",
    "        self.is_training = self.mode == \"training\"\n",
    "        self.reuse = self.mode == \"validation\"\n",
    "        \n",
    "        \n",
    "    def build_model(self, input_layer):\n",
    "        with tf.variable_scope(\"cnn_model\", reuse=self.reuse, initializer=self.config['initializer']):\n",
    "            # Convolutional Layer #1\n",
    "            # Computes 32 features using a 3x3 filter with ReLU activation.\n",
    "            # Padding is added to preserve width and height.\n",
    "            # Input Tensor Shape: [batch_size, 80, 80, num_channels]\n",
    "            # Output Tensor Shape: [batch_size, 40, 40, num_filter1]\n",
    "            conv1 = tf.layers.conv2d(\n",
    "                inputs=input_layer,\n",
    "                filters=self.config['cnn_filters'][0],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2, padding='same')\n",
    "\n",
    "            # Input Tensor Shape: [batch_size, 40, 40, num_filter1]\n",
    "            # Output Tensor Shape: [batch_size, 20, 20, num_filter2]\n",
    "            conv2 = tf.layers.conv2d(\n",
    "                inputs=pool1,\n",
    "                filters=self.config['cnn_filters'][1],\n",
    "                kernel_size=[5, 5],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2, padding='same')\n",
    "\n",
    "            # Input Tensor Shape: [batch_size, 20, 20, num_filter2]\n",
    "            # Output Tensor Shape: [batch_size, 10, 10, num_filter3]\n",
    "            conv3 = tf.layers.conv2d(\n",
    "                inputs=pool2,\n",
    "                filters=self.config['cnn_filters'][2],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2, padding='same')\n",
    "\n",
    "            # Input Tensor Shape: [batch_size, 10, 10, num_filter3]\n",
    "            # Output Tensor Shape: [batch_size, 5, 5, num_filter4]\n",
    "            conv4 = tf.layers.conv2d(\n",
    "                inputs=pool3,\n",
    "                filters=self.config['cnn_filters'][3],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "            pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2, padding='same')\n",
    "\n",
    "\n",
    "            # Flatten tensor into a batch of vectors\n",
    "            # Input Tensor Shape: [batch_size, 5, 5, num_filter4]\n",
    "            # Output Tensor Shape: [batch_size, 5 * 5 * num_filter4]\n",
    "            conv_flat = tf.reshape(pool4, [-1, 5 * 5 * self.config['cnn_filters'][3]])\n",
    "\n",
    "            # Add dropout operation;\n",
    "            dropout = tf.layers.dropout(inputs=conv_flat, rate=self.config['dropout_rate'], training=self.is_training)\n",
    "\n",
    "            # Dense Layer\n",
    "            # Densely connected layer with <num_hidden_units> neurons\n",
    "            # Input Tensor Shape: [batch_size, 5 * 5 * num_filter4]\n",
    "            # Output Tensor Shape: [batch_size, num_hidden_units]\n",
    "            dense = tf.layers.dense(inputs=dropout, units=self.config['num_hidden_units'],\n",
    "                activation=tf.nn.relu)\n",
    "        \n",
    "            self.cnn_model = dense\n",
    "            return dense\n",
    "            \n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "        CNNs accept inputs of shape (batch_size, height, width, num_channels). However, we have inputs of shape\n",
    "        (batch_size, sequence_length, height, width, num_channels) where sequence_length is inferred at run time.\n",
    "        We need to iterate in order to get CNN representations. Similar to python's map function, \"tf.map_fn\" \n",
    "        applies a given function on each entry in the input list. \n",
    "        \"\"\"\n",
    "        # For the first time create a dummy graph and then share the parameters everytime.\n",
    "        if self.is_training:\n",
    "            self.reuse = False\n",
    "            self.build_model(self.inputs[0])\n",
    "            self.reuse = True\n",
    "        \n",
    "        # CNN takes a clip as if it is a batch of samples.\n",
    "        # Have a look at tf.map_fn (https://www.tensorflow.org/api_docs/python/tf/map_fn)\n",
    "        # You can set parallel_iterations or swap_memory in order to make it faster.\n",
    "        # Note that back_prop argument is True in order to enable training of CNN.\n",
    "        self.cnn_representations = tf.map_fn(lambda x: self.build_model(x),\n",
    "                                                elems=self.inputs,\n",
    "                                                dtype=tf.float32,\n",
    "                                                back_prop=True,\n",
    "                                                swap_memory=True,\n",
    "                                                parallel_iterations=2)\n",
    "        \n",
    "        return self.cnn_representations\n",
    "        \n",
    "\n",
    "class RNNModel():\n",
    "    \"\"\"\n",
    "    Creates training and validation computational graphs.\n",
    "    Note that tf.variable_scope enables sharing the parameters so that both graphs share the parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, input_op, target_op, seq_len_op, mode):\n",
    "        \"\"\"\n",
    "        Basic setup.\n",
    "        Args:\n",
    "          config: Object containing configuration parameters.\n",
    "        \"\"\"\n",
    "        assert mode in [\"training\", \"validation\"]\n",
    "        self.config = config\n",
    "        self.inputs = input_op\n",
    "        self.targets = target_op\n",
    "        self.seq_lengths = seq_len_op\n",
    "        self.mode = mode\n",
    "        self.reuse = self.mode == \"validation\"\n",
    "        \n",
    "    def build_rnn_model(self):\n",
    "        with tf.variable_scope('rnn_cell', reuse=self.reuse, initializer=self.config['initializer']):\n",
    "            rnn_cell = tf.contrib.rnn.BasicLSTMCell(num_units=self.config['num_hidden_units'])\n",
    "        with tf.variable_scope('rnn_stack', reuse=self.reuse, initializer=self.config['initializer']):\n",
    "            if self.config['num_layers'] > 1:\n",
    "                rnn_cell = tf.contrib.rnn.MultiRNNCell([rnn_cell for _ in range(self.config['num_layers'])])\n",
    "            self.model_rnn, self.rnn_state = tf.nn.dynamic_rnn(\n",
    "                                            cell=rnn_cell,\n",
    "                                            inputs=self.inputs,\n",
    "                                            dtype = tf.float32,\n",
    "                                            sequence_length=self.seq_lengths,\n",
    "                                            time_major=False,\n",
    "                                            swap_memory=True)\n",
    "            # Fetch output of the last step.\n",
    "            if self.config['loss_type'] == 'last_step':\n",
    "                self.rnn_prediction = tf.gather_nd(self.model_rnn, tf.stack([tf.range(self.config['batch_size']), self.seq_lengths-1], axis=1))\n",
    "            elif self.config['loss_type'] == 'average':\n",
    "                self.rnn_prediction = self.model_rnn\n",
    "            else:\n",
    "                print(\"Invalid loss type\")\n",
    "                raise\n",
    "                \n",
    "    \n",
    "    def build_model(self):\n",
    "        self.build_rnn_model()\n",
    "        # Calculate logits\n",
    "        with tf.variable_scope('logits', reuse=self.reuse, initializer=self.config['initializer']):\n",
    "            self.logits = tf.layers.dense(inputs=self.rnn_prediction, units=self.config['num_class_labels'],\n",
    "                                            kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                            bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            # In the case of average loss, take average of time steps in order to calculate\n",
    "            # final prediction probabilities.\n",
    "            if self.config['loss_type'] == 'average':\n",
    "                self.logits = tf.reduce_mean(self.logits, axis=1)\n",
    "            \n",
    "    def loss(self):\n",
    "        # Loss calculations: cross-entropy\n",
    "        with tf.name_scope(\"cross_entropy_loss\"):\n",
    "            self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.targets))\n",
    "            \n",
    "                # Accuracy calculations.\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # Return list of predictions (useful for making a submission)\n",
    "            self.predictions = tf.argmax(self.logits, 1, name=\"predictions\")\n",
    "            # Return a bool tensor with shape [batch_size] that is true for the\n",
    "            # correct predictions.\n",
    "            self.correct_predictions = tf.equal(tf.argmax(self.logits, 1), self.targets)\n",
    "            # Number of correct predictions in order to calculate average accuracy afterwards.\n",
    "            self.num_correct_predictions = tf.reduce_sum(tf.cast(self.correct_predictions, tf.int32))\n",
    "            # Calculate the accuracy per minibatch.\n",
    "            self.batch_accuracy = tf.reduce_mean(tf.cast(self.correct_predictions, tf.float32))\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self.build_model()\n",
    "        self.loss()\n",
    "        self.num_parameters()\n",
    "        \n",
    "        return self.logits, self.loss, self.batch_accuracy, self.predictions\n",
    "    \n",
    "    def num_parameters(self):\n",
    "        self.num_parameters = 0\n",
    "        #iterating over all variables\n",
    "        for variable in tf.trainable_variables():\n",
    "            local_parameters=1\n",
    "            shape = variable.get_shape()  #getting shape of a variable\n",
    "            for i in shape:\n",
    "                local_parameters*=i.value  #mutiplying dimension values\n",
    "            self.num_parameters+=local_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# of parameters: 3853716\n"
     ]
    }
   ],
   "source": [
    "#reset\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create a list of tfRecord input files.\n",
    "train_filenames = [os.path.join(config['train_data_dir'], config['train_file_format'] % i) for i in config['train_file_ids']]\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "train_batch_samples_op, train_batch_labels_op, train_batch_seq_len_op = input_pipeline(train_filenames, config, name='training_input_pipeline')\n",
    "\n",
    "# Create a list of tfRecord input files.\n",
    "valid_filenames = [os.path.join(config['valid_data_dir'], config['valid_file_format'] % i) for i in config['valid_file_ids']]\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "valid_batch_samples_op, valid_batch_labels_op, valid_batch_seq_len_op = input_pipeline(valid_filenames, config, name='validation_input_pipeline', shuffle=False)\n",
    "\n",
    "# Create placeholders for training and monitoring variables.\n",
    "loss_avg_op = tf.placeholder(tf.float32, name=\"loss_avg\")\n",
    "accuracy_avg_op = tf.placeholder(tf.float32, name=\"accuracy_avg\")\n",
    "\n",
    "# Generate a variable to contain a counter for the global training step.\n",
    "# Note that it is useful if you save/restore your network.\n",
    "global_step = tf.Variable(1, name='global_step', trainable=False)\n",
    "\n",
    "# Create seperate graphs for training and validation.\n",
    "# Training graph\n",
    "# Note that our model is optimized by using the training graph.\n",
    "with tf.name_scope(\"Training\"):\n",
    "    # Create model\n",
    "    cnnModel = CNNModel(config=config['cnn'],\n",
    "                        input_op=train_batch_samples_op, \n",
    "                        mode='training')\n",
    "    cnn_representations = cnnModel.build_graph()\n",
    "    \n",
    "    trainModel = RNNModel(config=config['rnn'], \n",
    "                            input_op=cnn_representations, \n",
    "                            target_op=train_batch_labels_op, \n",
    "                            seq_len_op=train_batch_seq_len_op,\n",
    "                            mode=\"training\")\n",
    "    trainModel.build_graph()\n",
    "    print(\"\\n# of parameters: %s\" % trainModel.num_parameters)\n",
    "    \n",
    "    # Optimization routine.\n",
    "    # Learning rate is decayed in time. This enables our model using higher learning rates in the beginning.\n",
    "    # In time the learning rate is decayed so that gradients don't explode and training staurates.\n",
    "    # If you observe slow training, feel free to modify decay_steps and decay_rate arguments.\n",
    "    if config['learning_rate_type'] == 'exponential':\n",
    "        learning_rate = tf.train.exponential_decay(config['learning_rate'], \n",
    "                                                   global_step=global_step,\n",
    "                                                   decay_steps=1000, \n",
    "                                                   decay_rate=0.97,\n",
    "                                                   staircase=False)\n",
    "    elif config['learning_rate_type'] == 'fixed':\n",
    "        learning_rate = config['learning_rate']\n",
    "        momentum = config['momentum']\n",
    "    else:\n",
    "        print(\"Invalid learning rate type\")\n",
    "        raise\n",
    "        \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    train_op = optimizer.minimize(trainModel.loss, global_step=global_step)\n",
    "\n",
    "# Validation graph.\n",
    "with tf.name_scope(\"Evaluation\"):\n",
    "    # Create model\n",
    "    validCnnModel = CNNModel(config=config['cnn'],\n",
    "                                input_op=valid_batch_samples_op, \n",
    "                                mode='validation')\n",
    "    valid_cnn_representations = validCnnModel.build_graph()\n",
    "    \n",
    "    validModel = RNNModel(config=config['rnn'], \n",
    "                            input_op=valid_cnn_representations, \n",
    "                            target_op=valid_batch_labels_op, \n",
    "                            seq_len_op=valid_batch_seq_len_op,\n",
    "                            mode=\"validation\")\n",
    "    validModel.build_graph()\n",
    "\n",
    "    \n",
    "# Create summary ops for monitoring the training.\n",
    "# Each summary op annotates a node in the computational graph and collects\n",
    "# data data from it.\n",
    "summary_train_loss = tf.summary.scalar('loss', trainModel.loss)\n",
    "summary_train_acc = tf.summary.scalar('accuracy_training', trainModel.batch_accuracy)\n",
    "summary_avg_accuracy = tf.summary.scalar('accuracy_avg', accuracy_avg_op)\n",
    "summary_avg_loss = tf.summary.scalar('loss_avg', loss_avg_op)\n",
    "summary_learning_rate = tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "# Group summaries.\n",
    "# summaries_training is used during training and reported after every step.\n",
    "summaries_training = tf.summary.merge([summary_train_loss, summary_train_acc, summary_learning_rate])\n",
    "# summaries_evaluation is used by both trainig and validation in order to report the performance on the dataset.\n",
    "summaries_evaluation = tf.summary.merge([summary_avg_accuracy, summary_avg_loss])\n",
    "    \n",
    "#Create session object\n",
    "sess = tf.Session()\n",
    "# Add the ops to initialize variables.\n",
    "init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "# Actually intialize the variables\n",
    "sess.run(init_op)\n",
    "\n",
    "# Register summary ops.\n",
    "train_summary_dir = os.path.join(config['model_dir'], \"summary\", \"train\")\n",
    "train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "valid_summary_dir = os.path.join(config['model_dir'], \"summary\", \"validation\")\n",
    "valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
    "\n",
    "# Create a saver for writing training checkpoints.\n",
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "# Define counters in order to accumulate measurements.\n",
    "counter_correct_predictions_training = 0.0\n",
    "counter_loss_training = 0.0\n",
    "counter_correct_predictions_validation = 0.0\n",
    "counter_loss_validation = 0.0\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/357] [Training] Accuracy: 0.054, Loss: 2.995\n",
      "[2/714] [Training] Accuracy: 0.164, Loss: 2.639\n",
      "[3/1071] [Training] Accuracy: 0.249, Loss: 2.293\n",
      "[4/1428] [Training] Accuracy: 0.338, Loss: 2.015\n",
      "[5/1785] [Training] Accuracy: 0.419, Loss: 1.738\n",
      "[5/1785] [Validation] Accuracy: 0.432, Loss: 1.785\n",
      "[6/2142] [Training] Accuracy: 0.507, Loss: 1.470\n",
      "[7/2499] [Training] Accuracy: 0.576, Loss: 1.274\n",
      "[8/2856] [Training] Accuracy: 0.647, Loss: 1.067\n",
      "[9/3213] [Training] Accuracy: 0.699, Loss: 0.923\n",
      "Model saved in file: /home/arshan/project/codes/runs/1497438780/model-3570\n",
      "[10/3570] [Training] Accuracy: 0.754, Loss: 0.766\n",
      "[10/3570] [Validation] Accuracy: 0.562, Loss: 1.648\n",
      "[11/3927] [Training] Accuracy: 0.789, Loss: 0.664\n",
      "[12/4284] [Training] Accuracy: 0.821, Loss: 0.562\n",
      "[13/4641] [Training] Accuracy: 0.853, Loss: 0.474\n",
      "[14/4998] [Training] Accuracy: 0.875, Loss: 0.412\n",
      "[15/5355] [Training] Accuracy: 0.898, Loss: 0.334\n",
      "[15/5355] [Validation] Accuracy: 0.595, Loss: 1.873\n",
      "[16/5712] [Training] Accuracy: 0.909, Loss: 0.308\n",
      "[17/6069] [Training] Accuracy: 0.927, Loss: 0.247\n",
      "[18/6426] [Training] Accuracy: 0.923, Loss: 0.254\n",
      "[19/6783] [Training] Accuracy: 0.938, Loss: 0.211\n",
      "Model saved in file: /home/arshan/project/codes/runs/1497438780/model-7140\n",
      "[20/7140] [Training] Accuracy: 0.929, Loss: 0.241\n",
      "[20/7140] [Validation] Accuracy: 0.614, Loss: 1.904\n",
      "[21/7497] [Training] Accuracy: 0.953, Loss: 0.173\n",
      "[22/7854] [Training] Accuracy: 0.949, Loss: 0.165\n",
      "[23/8211] [Training] Accuracy: 0.954, Loss: 0.160\n",
      "[24/8568] [Training] Accuracy: 0.961, Loss: 0.137\n",
      "[25/8925] [Training] Accuracy: 0.949, Loss: 0.177\n",
      "[25/8925] [Validation] Accuracy: 0.605, Loss: 2.113\n",
      "[26/9282] [Training] Accuracy: 0.961, Loss: 0.131\n",
      "[27/9639] [Training] Accuracy: 0.963, Loss: 0.126\n",
      "[28/9996] [Training] Accuracy: 0.967, Loss: 0.119\n",
      "[29/10353] [Training] Accuracy: 0.963, Loss: 0.134\n",
      "Model saved in file: /home/arshan/project/codes/runs/1497438780/model-10710\n",
      "[30/10710] [Training] Accuracy: 0.967, Loss: 0.124\n",
      "[30/10710] [Validation] Accuracy: 0.619, Loss: 2.186\n",
      "[31/11067] [Training] Accuracy: 0.977, Loss: 0.087\n",
      "[32/11424] [Training] Accuracy: 0.974, Loss: 0.101\n",
      "[33/11781] [Training] Accuracy: 0.976, Loss: 0.092\n",
      "[34/12138] [Training] Accuracy: 0.970, Loss: 0.109\n",
      "[35/12495] [Training] Accuracy: 0.984, Loss: 0.058\n",
      "[35/12495] [Validation] Accuracy: 0.628, Loss: 2.407\n",
      "[36/12852] [Training] Accuracy: 0.979, Loss: 0.075\n",
      "[37/13209] [Training] Accuracy: 0.970, Loss: 0.103\n",
      "[38/13566] [Training] Accuracy: 0.956, Loss: 0.144\n",
      "[39/13923] [Training] Accuracy: 0.973, Loss: 0.092\n",
      "Model saved in file: /home/arshan/project/codes/runs/1497438780/model-14280\n",
      "[40/14280] [Training] Accuracy: 0.980, Loss: 0.076\n",
      "[40/14280] [Validation] Accuracy: 0.600, Loss: 2.381\n",
      "[41/14637] [Training] Accuracy: 0.990, Loss: 0.045\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4b7759fc19c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                                                                       \u001b[0mtrainModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                                                       train_op], \n\u001b[0;32m---> 18\u001b[0;31m                                                                       feed_dict={})\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Update counters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcounter_correct_predictions_training\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_correct_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "try:\n",
    "    while not coord.should_stop():\n",
    "        step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "        if (step%config['checkpoint_every_step']) == 0:\n",
    "            ckpt_save_path = saver.save(sess, os.path.join(config['model_dir'], 'model'), global_step)\n",
    "            print(\"Model saved in file: %s\" % ckpt_save_path)\n",
    "            \n",
    "        # Run the optimizer to update weights.\n",
    "        # Note that \"train_op\" is responsible from updating network weights.\n",
    "        # Only the operations that are fed are evaluated.\n",
    "        # Run the optimizer to update weights.\n",
    "        train_summary, num_correct_predictions, loss, _ = sess.run([summaries_training, \n",
    "                                                                      trainModel.num_correct_predictions, \n",
    "                                                                      trainModel.loss, \n",
    "                                                                      train_op], \n",
    "                                                                      feed_dict={})\n",
    "        # Update counters.\n",
    "        counter_correct_predictions_training += num_correct_predictions\n",
    "        counter_loss_training += loss\n",
    "        # Write summary data.\n",
    "        train_summary_writer.add_summary(train_summary, step)\n",
    "        \n",
    "        # Report training performance\n",
    "        if (step%config['print_every_step']) == 0:\n",
    "            accuracy_avg = counter_correct_predictions_training / (config['batch_size']*config['print_every_step'])\n",
    "            loss_avg = counter_loss_training / (config['print_every_step'])\n",
    "            summary_report = sess.run(summaries_evaluation, feed_dict={accuracy_avg_op:accuracy_avg, loss_avg_op:loss_avg})\n",
    "            train_summary_writer.add_summary(summary_report, step)\n",
    "            print(\"[%d/%d] [Training] Accuracy: %.3f, Loss: %.3f\" % (step/config['num_steps_per_epoch'], \n",
    "                                                                     step, \n",
    "                                                                     accuracy_avg, \n",
    "                                                                     loss_avg))\n",
    "            \n",
    "            counter_correct_predictions_training = 0.0\n",
    "            counter_loss_training= 0.0\n",
    "        \n",
    "        if (step%config['evaluate_every_step']) == 0:\n",
    "            # It is possible to create only one input pipelene queue. Hence, we create a validation queue \n",
    "            # in the begining for multiple epochs and control it via a foor loop.\n",
    "            # Note that we only approximate 1 validation epoch (validation doesn't have to be accurate.)\n",
    "            # In other words, number of unique validation samples may differ everytime.\n",
    "            for eval_step in range(config['num_validation_steps']):\n",
    "                # Calculate average validation accuracy.\n",
    "                num_correct_predictions, loss = sess.run([validModel.num_correct_predictions, \n",
    "                                                          validModel.loss],\n",
    "                                                         feed_dict={})\n",
    "                # Update counters.\n",
    "                counter_correct_predictions_validation += num_correct_predictions\n",
    "                counter_loss_validation += loss\n",
    "            \n",
    "            # Report validation performance\n",
    "            accuracy_avg = counter_correct_predictions_validation / (config['batch_size']*config['num_validation_steps'])\n",
    "            loss_avg = counter_loss_validation / (config['num_validation_steps'])\n",
    "            summary_report = sess.run(summaries_evaluation, feed_dict={accuracy_avg_op:accuracy_avg, loss_avg_op:loss_avg})\n",
    "            valid_summary_writer.add_summary(summary_report, step)\n",
    "            print(\"[%d/%d] [Validation] Accuracy: %.3f, Loss: %.3f\" % (step/config['num_steps_per_epoch'], \n",
    "                                                                       step, \n",
    "                                                                       accuracy_avg, \n",
    "                                                                       loss_avg))\n",
    "            \n",
    "            counter_correct_predictions_validation = 0.0\n",
    "            counter_loss_validation= 0.0\n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Model is trained for %d epochs, %d steps.' % (config['num_epochs'], step))\n",
    "    print('Done.')\n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()\n",
    "\n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)\n",
    "\n",
    "ckpt_save_path = saver.save(sess, os.path.join(config['model_dir'], 'model'), global_step)\n",
    "print(\"Model saved in file: %s\" % ckpt_save_path)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A simple sanity check for input pipeline:\n",
    "'''\n",
    "def test_input_loader():\n",
    "    config = {}\n",
    "    config['img_height'] = 80\n",
    "    config['img_width'] = 80\n",
    "    config['img_num_channels'] = 3\n",
    "    config['num_epochs'] = 10\n",
    "    config['batch_size'] = 16\n",
    "    # Capacity of the queue which contains the samples read by data readers.\n",
    "    # Make sure that it has enough capacity.\n",
    "    config['ip_queue_capacity'] = config['batch_size']*10  \n",
    "    config['ip_num_read_threads'] = 6\n",
    "    # Directory of the data.\n",
    "    config['data_dir'] = \"/home/eaksan/uie_data/train/\"\n",
    "    # File naming\n",
    "    config['file_format'] = \"dataTrain_%d.tfrecords\"\n",
    "    # File IDs to be used for training.\n",
    "    config['file_ids'] = list(range(1,10))\n",
    "    \n",
    "    # Create a list of TFRecord input files.\n",
    "    filenames = [os.path.join(config['data_dir'], config['file_format'] % i) for i in config['file_ids']]\n",
    "\n",
    "    # Create data loading operators. This will be represented as a node in the computational graph.\n",
    "    batch_samples_op, batch_labels_op, batch_seq_len_op = input_pipeline(filenames, config)\n",
    "    # TODO: batch_samples_op, batch_labels_op and batch_seq_len_op are like input placeholders. You can directly \n",
    "    # feed them to your model.\n",
    "\n",
    "    # Create tensorflow session and initialize the variables (if any).\n",
    "    sess = tf.Session()\n",
    "    init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    # Create threads to prefetch the data.\n",
    "    # https://www.tensorflow.org/programmers_guide/reading_data#creating_threads_to_prefetch_using_queuerunner_objects\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    batch_samples, batch_labels, batch_seq_len = sess.run([batch_samples_op, batch_labels_op, batch_seq_len_op])\n",
    "    \n",
    "    # Print \n",
    "    print(\"# Samples: \" + str(len(batch_samples)))\n",
    "    print(\"Sequence lengths: \" + str(batch_seq_len))\n",
    "    print(\"Sequence labels: \" + str(batch_labels))\n",
    "    \n",
    "    # Note that the second dimension will give maximum-length in the batch, i.e., the padded sequence length.\n",
    "    print(\"Sequence type: \" + str(type(batch_samples)))\n",
    "    print(\"Sequence shape: \" + str(batch_samples.shape))\n",
    "\n",
    "    # Fetch first clips 11th frame.\n",
    "    img = batch_samples[0][10]\n",
    "    print(\"(flattened) Image shape: \" + str(img.shape))\n",
    "    img = np.reshape(img, (32,32))\n",
    "    print(\"Image shape: \" + str(img.shape))\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img) # Note that image may look wierd because it is normalized.\n",
    "    \n",
    "test_input_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
